{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction To Optimal Growth: Cake Eating Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Introduction To Optimal Growth: Cake Eating Problem](#Introduction-To-Optimal-Growth:-Cake-Eating-Problem)  \n",
    "  - [Overview](#Overview)  \n",
    "  - [The Model](#The-Model)  \n",
    "  - [Value Function](#Value-Function)  \n",
    "  - [Value Function Iteration](#Value-Function-Iteration)  \n",
    "  - [Implementation](#Implementation)  \n",
    "  - [Analytical Solution](#Analytical-Solution)  \n",
    "  - [Policy Function](#Policy-Function)  \n",
    "  - [Exercises](#Exercises)  \n",
    "  - [Solutions](#Solutions)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to what’s in Anaconda, this lecture will require the following library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this lecture we will consider a simple optimal growth model with one agent and captial with no growth and perfect transformation into output. It is often referred to as the cake eating problem.\n",
    "\n",
    "On the surface it may seem a simple problem.\n",
    "How much of a cake to eat today and leave for tomorrow?\n",
    "\n",
    "However it quickly becomes an interesting dynamic programming problem.\n",
    "\n",
    "This problem is a good introduction to the stochastic optimal growth problem which we look at in later lectures.\n",
    "\n",
    "We require the following imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from interpolation import interp\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "We are operating on a infinite time horizen $ t=0,1,2,3... $\n",
    "\n",
    "At $ t=0 $ the agent is given the perfect cake with size $ \\bar{y} $, this is exogenously given.\n",
    "\n",
    "At the beginning of each period the size of the cake is $ y_t $, with $ y_0=\\bar{y} $.\n",
    "\n",
    "We are asked to choose how much of the cake to eat in any given period $ t $.\n",
    "\n",
    "We will assume that consuming $ c $ of the cake gives the agent utility in the form of the CRRA utility function.\n",
    "\n",
    "$$\n",
    "u(c) = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            \\frac{c^{1-\\gamma}}{1-\\gamma}& \\quad \\text{for all}\\ \\gamma \\neq 1 \\text{and } \\gamma\\geq 0\\\\\n",
    "            \\ln(c) & \\quad \\gamma = 1\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "After choosing to consumed $ c_t $ of the cake in period $ t $ there is\n",
    "\n",
    "$$\n",
    "y_{t+1} = y_t - c_t \\ \\text{for all}\\ t\n",
    "$$\n",
    "\n",
    "left in period $ t+1 $.\n",
    "\n",
    "Future cake consumption utility is discounted by $ \\beta\\in(0,1) $.\n",
    "\n",
    "The agent’s problem can be written as\n",
    "\n",
    "$$\n",
    "\\max_{\\{c_t\\}_{t = 0}^{t = \\infty}} \\sum_{t=0}^{\\infty} \\beta^t u(c_t)\n",
    "$$\n",
    "\n",
    "subject to.\n",
    "\n",
    "$$\n",
    "y_{t+1} = y_t - c_t \\ \\text{for all}\\ t\\text{,}\\\\\n",
    "0\\leq c_t\\leq y_t\\ \\text{for all}\\ t\\text{,}\\\\\n",
    "y_0=\\bar{y}\\text{.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trade-Off\n",
    "\n",
    "Our agent is faced with the trade offs:\n",
    "* Eating more today to avoid the discount factor.\n",
    "* Eating more tomorrow with the agents preference aiming to smooth consumption, due to the risk aversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function\n",
    "\n",
    "We can break down the the agents problem in terms of these two trade-offs.\n",
    "\n",
    "As such we can set up the agent’s problem to look like.\n",
    "\n",
    "\n",
    "<a id='equation-value-fun'></a>\n",
    "$$\n",
    "v(y_0) = \\max_{\\{c_t\\}_{t = 0}^{t = \\infty}\\\\0\\leq c_{t}\\leq y_{t}} \\sum_{t=0}^{\\infty} \\beta^t u(c_t) \\tag{1}\n",
    "$$\n",
    "\n",
    "$ v(y_0) $, our value function, can be interpreted as the agents total lifetime utility when optimially choosing $ \\{c_t\\}_{t = 0}^{t = \\infty} $ for a given $ y_0 $.\n",
    "\n",
    "Breaking [(1)](#equation-value-fun) down into the two trade-offs gives.\n",
    "\n",
    "\n",
    "<a id='equation-tra-value-fun'></a>\n",
    "$$\n",
    "v(y_0) = \\max_{0\\leq c_0\\leq y_0}\n",
    "    \\left\\{\n",
    "        u(c_0) +\n",
    "        \\beta\\max_{\\{c_t\\}_{t = 1}^{t = \\infty}\\\\0\\leq c_{t}\\leq y_{t}} \\sum_{t=1}^{\\infty} \\beta^{t-1} u(c_t)\n",
    "    \\right\\} \\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bellman Equation\n",
    "\n",
    "[(2)](#equation-tra-value-fun) yields the bellman equation.\n",
    "\n",
    "$$\n",
    "v(y_0) = \\max_{0\\leq c_0\\leq y_0}\\{u(c_0) + \\beta v(y_1)\\}\\\\\n",
    "v(y_t) = \\max_{0\\leq c_t\\leq y_t}\\{u(c_t) + \\beta v(y_{t+1})\\}\n",
    "$$\n",
    "\n",
    "Here $ y_t $ is the state variable, with the transtion of the state variable depending on\n",
    "\n",
    "\n",
    "<a id='equation-state-tran'></a>\n",
    "$$\n",
    "y_{t+1} = y_t - c_t \\ \\text{for all}\\ t \\tag{3}\n",
    "$$\n",
    "\n",
    "As in previous dynamic programming lectures(LINK) we will use the Bellman operator \\$T\\$ to solve the Bellman equation.\n",
    "Any fixed point of \\$T\\$ solves the Bellman equation and vice versa.\n",
    "\n",
    "$$\n",
    "Tv(y) = \\max_{0 \\leq c \\leq y}\\{u(c) + \\beta v(y')\\}\n",
    "$$\n",
    "\n",
    "From [(3)](#equation-state-tran) we can re-write this as\n",
    "\n",
    "\n",
    "<a id='equation-bellman-val'></a>\n",
    "$$\n",
    "Tv(y) = \\max_{0 \\leq c \\leq y}\\{u(c) + \\beta v(y - c)\\} \\tag{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function Iteration\n",
    "\n",
    "In order to determine the value function we need to:\n",
    "\n",
    "1. Take an arbitary intial guess of $ v' $.  \n",
    "1. Plug $ v' $ into the right hand side of [(4)](#equation-bellman-val), find and store \\$c\\$ and \\$v\\$.  \n",
    "1. Unless a condition is met, set $ v'=v $ and go back to step 2.  \n",
    "\n",
    "\n",
    "As consumption choice $ c $ is a continous variable, the state variable $ y $ is continous. This makes things tricky.\n",
    "\n",
    "In order to determine $ v $ we have to store every $ v(y) $ for every $ y\\in [0,\\bar{y}] $, which is difficult given there are infinitly many points.\n",
    "\n",
    "To get around this we’ll create a finite grid of different size cakes $ \\bar{y}=y_0>y_1>y_2>...y_I>0 $ and determine the $ v $ for each point on the grid and store them.\n",
    "\n",
    "The process looks like:\n",
    "\n",
    "1. Begin with an array of values $ \\{ v_0, \\ldots, v_I \\} $  representing\n",
    "  the values of some initial function \\$ v \\$ on the grid points $ \\{ y_0, \\ldots, y_I \\} $.  \n",
    "1. Build a function $ \\hat v $ on the state space $ \\mathbb R_+ $ by\n",
    "  linear interpolation, based on these data points.  \n",
    "1. Obtain and record the value $ T \\hat v(y_i) $ on each grid point\n",
    "  $ y_i $ by repeatedly solving.  \n",
    "1. Unless some stopping condition is satisfied, set\n",
    "  $ \\{ v_0, \\ldots, v_I \\} = \\{ T \\hat v(y_0), \\ldots, T \\hat v(y_I) \\} $ and go to step 2.  \n",
    "\n",
    "\n",
    "In step 2 we’ll use the same continuous piecewise linear interpolation strategy as is the previous lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Firstly we need to be able to find both the maximum and the maximizer of the value function. However scipy only has a `minimize_scalar` function which finds the minimum and the minimizer of a function on a certain bound.\n",
    "\n",
    "In order find the maximum of the value function we have to take the negative of the value function and find its minimum and minimizer with `minimize_scalar`.\n",
    "\n",
    "The `maximize` function below, takes a function `g` and does just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def maximize(g, a, b, args):\n",
    "    \"\"\"\n",
    "    Maximize the function g over the interval [a, b].\n",
    "\n",
    "    We use the fact that the maximizer of g on any interval is\n",
    "    also the minimizer of -g.  The tuple args collects any extra\n",
    "    arguments to g.\n",
    "\n",
    "    Returns the maximal value and the maximizer.\n",
    "    \"\"\"\n",
    "\n",
    "    objective = lambda x: -g(x, *args)\n",
    "    result = minimize_scalar(objective, bounds=(a, b), method='bounded')\n",
    "    maximizer, maximum = result.x, -result.fun\n",
    "    return maximizer, maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll store the primitives such as $ \\beta $ and $ \\gamma $ in the class `EatCake`.\n",
    "\n",
    "This class will also have a function which returns the right hand right of the bellman equation which needs to be maximized, which is the function that will run through the `maximize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class EatCake:\n",
    "\n",
    "    def __init__(self,\n",
    "                β=0.96,       # discount factor\n",
    "                γ=0.5,        # degree of relative risk aversion\n",
    "                y_grid_max=10,  # inital stock of capital Y\n",
    "                y_grid_size=120):\n",
    "\n",
    "        self.β, self.γ = β, γ\n",
    "\n",
    "        # Set up grid\n",
    "        self.y_grid = np.linspace(1e-04, y_grid_max, y_grid_size)\n",
    "\n",
    "    # Utility function\n",
    "    def u(self, c):\n",
    "\n",
    "        if self.γ == 1:\n",
    "            return np.log(c)\n",
    "        else:\n",
    "            return (c**(1 - self.γ)) / (1 - self.γ)\n",
    "\n",
    "\n",
    "    def state_action_value(self, c, y, v_array):\n",
    "        \"\"\"\n",
    "        Right hand side of the Bellman equation.\n",
    "        \"\"\"\n",
    "\n",
    "        u, β = self.u, self.β\n",
    "\n",
    "        v = lambda x: interp(self.y_grid, v_array, x)\n",
    "\n",
    "        return u(c) + β * v(y - c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the Bellman operator to determine the value function for each point on the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def T(ec, v):\n",
    "    \"\"\"\n",
    "    The Bellman operator.  Updates the guess of the value function.\n",
    "\n",
    "    * ec is an instance of EatCake\n",
    "    * v is an array representing a guess of the value function\n",
    "\n",
    "    \"\"\"\n",
    "    v_new = np.empty_like(v)\n",
    "\n",
    "    for i in range(len(ec.y_grid)):\n",
    "        y = ec.y_grid[i]\n",
    "        # Maximize RHS of Bellman equation at state y\n",
    "        v_max = maximize(ec.state_action_value, 1e-10, y, (y, v))[1]\n",
    "        v_new[i] = v_max\n",
    "\n",
    "\n",
    "    return v_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let’s create an instance of the model and assign it to the variable ec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ec = EatCake()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see the iteration of the value function in action. The intial guess will be \\$0\\$ for every \\$y\\$ point.\n",
    "\n",
    "We should see that the value functions converge to a single function as the intial guess is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "y_grid = ec.y_grid\n",
    "v = np.zeros(len(y_grid))  # Initial guess\n",
    "n = 35                   # Number of iterations\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(y_grid, v, color=plt.cm.jet(0),\n",
    "        lw=2, alpha=0.6, label='Initial condition')\n",
    "\n",
    "for i in range(n):\n",
    "    v = T(ec, v)  # Apply the Bellman operator\n",
    "    ax.plot(y_grid, v, color=plt.cm.jet(i / n), lw=2, alpha=0.6)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylabel('$v(y)$', fontsize=12)\n",
    "ax.set_xlabel('$y$', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can return the converged function by making a function `value_fun` that returns the value function once there has been sufficent convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def value_fun(ec,\n",
    "        tol=1e-4,\n",
    "        max_iter=1000,\n",
    "        verbose=True,\n",
    "        print_skip=25):\n",
    "\n",
    "    # Set up loop\n",
    "    v = np.zeros(len(ec.y_grid)) # Initial condition\n",
    "    i = 0\n",
    "    error = tol + 1\n",
    "\n",
    "    while i < max_iter and error > tol:\n",
    "        v_new = T(ec, v)\n",
    "        error = np.max(np.abs(v - v_new))\n",
    "        i += 1\n",
    "        if verbose and i % print_skip == 0:\n",
    "            print(f\"Error at iteration {i} is {error}.\")\n",
    "        v = v_new\n",
    "\n",
    "    if i == max_iter:\n",
    "        print(\"Failed to converge!\")\n",
    "\n",
    "    if verbose and i < max_iter:\n",
    "        print(f\"\\nConverged in {i} iterations.\")\n",
    "\n",
    "    return v_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "v_solution = value_fun(ec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot and see what the value and policy functions look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(y_grid, v_solution, lw=2, alpha=0.6,\n",
    "        label='Approximate value function')\n",
    "\n",
    "ax.set_ylabel('$v^*(y)$', fontsize=12)\n",
    "ax.set_xlabel('$y$', fontsize=12)\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Solution\n",
    "\n",
    "This problem has an analytical solution.\n",
    "We can check the results from our code matches up with the analytical solution.\n",
    "\n",
    "For a CRRA utility function the solution for the value function is:\n",
    "\n",
    "$$\n",
    "v^*(y) = \\left(1-\\beta^{\\frac{1}{\\gamma}}\\right)^{-\\gamma}u(y)\n",
    "$$\n",
    "\n",
    "We leave the proof as an exercise for the reader.\n",
    "\n",
    "The function below returns the value function for a given y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def v_star(y, u=ec.u, β=ec.β, γ=ec.γ):\n",
    "\n",
    "    a = β**(1/γ)\n",
    "    x = 1-a\n",
    "    z = u(y)\n",
    "\n",
    "    return z*x**(-γ)\n",
    "\n",
    "v_star = v_star(u=ec.u, y=y_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "ax.plot(y_grid, v_star, lw=2,\n",
    "        alpha=0.6, label='True value function')\n",
    "\n",
    "ax.plot(y_grid, v_solution, lw=2,\n",
    "        alpha=0.6, label='Approximate value function')\n",
    "\n",
    "ax.set_ylabel('$v^*(y)$', fontsize=12)\n",
    "ax.set_xlabel('$y$', fontsize=12)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray! It looks like our code is pretty close to the analytical solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Function\n",
    "\n",
    "Now we have the converged value function it is easy for us to determine the optimal consumption of the cake.\n",
    "\n",
    "This optimal choice of $ \\{c_t\\}_{t = 0}^{t = \\infty} $ is often referred to as the agents policy function.\n",
    "\n",
    "It is the mapping from the state space to the action space which maximises the agents lifetime utility.\n",
    "\n",
    "$$\n",
    "c^*_t = \\sigma(y_t) = \\arg \\max_{y_t} v(y_t)\n",
    "$$\n",
    "\n",
    "We can you the converged value function in the right hand side of the bellman equation and find the maximizer at every point on the grid. This will return us the optimal consumption $ c^* $ for a given state $ y $.\n",
    "\n",
    "Below is a similar function to the bellman operator function above. It finds the maximizer for every point on the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def σ(ec, v):\n",
    "    \"\"\"\n",
    "    The Bellman operator.  Updates the guess of the value function\n",
    "    and also computes a c_new policy.\n",
    "\n",
    "    * ec is an instance of EatCake\n",
    "    * v is an array representing a guess of the value function\n",
    "\n",
    "    \"\"\"\n",
    "    c_new = np.empty_like(v)\n",
    "\n",
    "\n",
    "    for i in range(len(ec.y_grid)):\n",
    "        y = ec.y_grid[i]\n",
    "        # Maximize RHS of Bellman equation at state y\n",
    "        c_max = maximize(ec.state_action_value, 1e-10, y, (y, v))[0]\n",
    "        c_new[i] = c_max\n",
    "\n",
    "\n",
    "    return c_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the converged value function in `σ` and get the immediate solution without iterating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "v_greedy = σ(ec, v_solution)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "ax.plot(y_grid, v_greedy, lw=2, alpha=0.6)\n",
    "\n",
    "ax.set_ylabel('$\\sigma(y)$', fontsize=12)\n",
    "ax.set_xlabel('$y$', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='pol-an'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical Solution\n",
    "\n",
    "We can compare the policy function our code gives us with the analytical solution.\n",
    "\n",
    "For this problem the analytical solution is\n",
    "\n",
    "$$\n",
    "c^* = \\left(1-\\beta^\\frac{1}{\\gamma}\\right)y\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def c_star(y, β=ec.β, γ=ec.γ):\n",
    "\n",
    "    return y*(1-β**(1/γ))\n",
    "c_star = c_star(y_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(y_grid, c_star, lw=2, alpha=0.6, label='True policy')\n",
    "\n",
    "ax.plot(y_grid, v_greedy, lw=2, alpha=0.6, label='Approximate policy')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylabel('$\\sigma(y)$', fontsize=12)\n",
    "ax.set_xlabel('$y$', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Prove that the optimal policy function is linear and there exists an postive $ \\theta $ such that $ c_t^*=\\theta y_t $\n",
    "\n",
    "(might change this to verify the value function above is the value function?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "In our example above we assumed that the production function of captial was $ f(k)=k $ because we were talking specficially about a cake.\n",
    "\n",
    "Now assume that the production function is in the form of $ f(k)=k^{\\alpha} $ where $ \\alpha\\in(0,1) $\n",
    "\n",
    "Make the required changes to the code above and plot the value and policy functions. Comment on the change in the policy function.\n",
    "\n",
    "Note $ y_t=f(k_t) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Suppose that the optimal policy is $ c_t^*=\\theta y_t $\n",
    "\n",
    "then\n",
    "\n",
    "$$\n",
    "y_{t+1}=y_t(1-\\theta)\n",
    "$$\n",
    "\n",
    "which means\n",
    "\n",
    "$$\n",
    "y_t = y_{0}(1-\\theta)^t\n",
    "$$\n",
    "\n",
    "Thus the optimal value function is.\n",
    "\n",
    "$$\n",
    "v^*(y_0) = \\sum_{t=0}^{\\infty} \\beta^{t} u(c_t)\\\\\n",
    "v^*(y_0) = \\sum_{t=0}^{\\infty} \\beta^{t} u(\\theta y_{t})\\\\\n",
    "v^*(y_0) = \\sum_{t=0}^{\\infty} \\beta^{t} u\\left(\\theta y_{0}(1-\\theta)^t\\right)\\\\\n",
    "v^*(y_0) = \\sum_{t=0}^{\\infty} \\theta^{1-\\gamma}\\beta^{t} (1-\\theta)^{t(1-\\gamma)}u(y_{0})\\\\\n",
    "v^*(y_0) = \\frac{\\theta^{1-\\gamma}}{1-\\beta(1-\\theta)^{1-\\gamma}}u(y_{0})\n",
    "$$\n",
    "\n",
    "Now with the optimal form of the value funciton we can impliment it in to the bellman equation.\n",
    "\n",
    "$$\n",
    "v(y) = \\max_{0\\leq c\\leq y}\n",
    "    \\left\\{\n",
    "        u(c) +\n",
    "        \\beta\\frac{\\theta^{1-\\gamma}}{1-\\beta(1-\\theta)^{1-\\gamma}}\\cdot u(y-c)\n",
    "    \\right\\}\\\\\n",
    "v(y) = \\max_{0\\leq c\\leq y}\n",
    "\\left\\{\n",
    "    \\frac{c^{1-\\gamma}}{1-\\gamma} +\n",
    "    \\beta\\frac{\\theta^{1-\\gamma}}{1-\\beta(1-\\theta)^{1-\\gamma}}\\cdot\\frac{(y-c)^{1-\\gamma}}{1-\\gamma}\n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "taking the F.O.C we have\n",
    "\n",
    "$$\n",
    "c^{-\\gamma} + \\beta\\frac{\\theta^{1-\\gamma}}{1-\\beta(1-\\theta)^{1-\\gamma}}\\cdot(y-c)^{-\\gamma}(-1) = 0\\\\\n",
    "c^{-\\gamma} = \\beta\\frac{\\theta^{1-\\gamma}}{1-\\beta(1-\\theta)^{1-\\gamma}}\\cdot(y-c)^{-\\gamma}\n",
    "$$\n",
    "\n",
    "with $ c = \\theta y $ we get\n",
    "\n",
    "$$\n",
    "\\left(\\theta y\\right)^{-\\gamma} =  \\beta\\frac{\\theta^{1-\\gamma}}{1-\\beta(1-\\theta)^{1-\\gamma}}\\cdot(y(1-\\theta))^{-\n",
    "\\gamma}\n",
    "$$\n",
    "\n",
    "With some re-arrangment we get\n",
    "\n",
    "$$\n",
    "\\theta = 1-\\beta^{\\frac{1}{\\gamma}}\n",
    "$$\n",
    "\n",
    "this gives the optimal policy of\n",
    "\n",
    "$$\n",
    "c_t^* = \\left(1-\\beta^{\\frac{1}{\\gamma}}\\right)y_t\n",
    "$$\n",
    "\n",
    "substituting $ \\theta $ into the value function above gives.\n",
    "\n",
    "$$\n",
    "v^*(y_t) = \\frac{\\left(1-\\beta^{\\frac{1}{\\gamma}}\\right)^{1-\\gamma}}{1-\\beta\\left(\\beta^{\\frac{{1-\\gamma}}{\\gamma}}\\right)}u(y_{t})\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "v^*(y_t) = \\left(1-\\beta^\\frac{1}{\\gamma}\\right)^{-\\gamma}u(y_t)\n",
    "$$\n",
    "\n",
    "Now we must verify that this value function is a fixed point, using the bellman equation.\n",
    "\n",
    "$$\n",
    "v(y) = \\max_{0\\leq c\\leq y}\n",
    "    \\left\\{\n",
    "        u(c) +\n",
    "        \\beta\\left(1-\\beta^\\frac{1}{\\gamma}\\right)^{-\\gamma}u(y-c)\n",
    "    \\right\\}\\\\\n",
    "$$\n",
    "\n",
    "taking the F.O.C we have\n",
    "\n",
    "$$\n",
    "c^{-\\gamma} - \\beta\\left(1-\\beta^\\frac{1}{\\gamma}\\right)^{-\\gamma}(y-c)^{-\\gamma} = 0\n",
    "$$\n",
    "\n",
    "re-arraning this gives\n",
    "\n",
    "$$\n",
    "c_t^* = \\left(1-\\beta^{\\frac{1}{\\gamma}}\\right)y_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "First note the resource contraint binds\n",
    "\n",
    "$$\n",
    "k_{t+1}=y_t-c_t\\ \\text{for all}\\ t\n",
    "$$\n",
    "\n",
    "from the production function output tomorrow is.\n",
    "\n",
    "$$\n",
    "y_{t+1}=f(y_t-c_t)\\ \\text{for all}\\ t\n",
    "$$\n",
    "\n",
    "We need to create a class to hold our primitives and return the right hand side of the bellman equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "class OptimalGrowth:\n",
    "\n",
    "    def __init__(self,\n",
    "                β=0.96,       # discount factor\n",
    "                γ=0.5,        # degree of relative risk aversion\n",
    "                α=0.4,\n",
    "                y_grid_max=10,  # inital stock of capital Y\n",
    "                y_grid_size=120):\n",
    "\n",
    "        self.β, self.γ, self.α = β, γ, α\n",
    "\n",
    "        # Set up grid\n",
    "        self.y_grid = np.linspace(1e-04, y_grid_max, y_grid_size)\n",
    "\n",
    "    def u(self, c):\n",
    "\n",
    "        if self.γ == 1:\n",
    "            return np.log(c)\n",
    "        else:\n",
    "            return (c**(1 - self.γ)) / (1 - self.γ)\n",
    "    def f(self, k):\n",
    "        return k**self.α\n",
    "\n",
    "    def state_action_value(self, c, y, v_array):\n",
    "\n",
    "        u, f, β = self.u, self.f, self.β\n",
    "\n",
    "        v = lambda x: interp(self.y_grid, v_array, x)\n",
    "\n",
    "        return u(c) + β * v(f(y - c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "og = OptimalGrowth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I’ll graph the iterations in of the value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "v = value_fun(og, verbose=False)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "ax.plot(y_grid, v, lw=2, alpha=0.6)\n",
    "ax.set_ylabel('v*(y)', fontsize=12)\n",
    "ax.set_xlabel('y', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "c_new = σ(og, v)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(y_grid, c_new,lw=2, alpha=0.6)\n",
    "\n",
    "ax.set_ylabel('$\\sigma(y)$', fontsize=12)\n",
    "ax.set_xlabel('$y$', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slope of the policy function has increased from what we saw [above](#pol-an).\n",
    "\n",
    "Because there is diminishing returns to capital and there is no growth in captial. The agent wants to eat more today to avoid the shrinking of the cake tomorrow."
   ]
  }
 ],
 "metadata": {
  "date": 1584147215.9010313,
  "filename": "cake_eating_problem.rst",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Introduction To Optimal Growth: Cake Eating Problem"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}